{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da243b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dipy \n",
    "from glob import glob\n",
    "\n",
    "#Get all summed_*.trk from /home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/\n",
    "bundles = glob('/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/summed_*.trk')\n",
    "\n",
    "cst_right = [bundles for bundles in bundles if 'CST_right' in bundles][0]\n",
    "cst_left = [bundles for bundles in bundles if 'CST_left' in bundles][0]\n",
    "\n",
    "print(cst_right)\n",
    "print(cst_left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find all thresholds that generate exactly 3 clusters\n",
    "def find_all_thresholds_for_3_clusters(streamlines, max_threshold=30, step=0.5):\n",
    "    working_thresholds = []\n",
    "    \n",
    "    for threshold in np.arange(step+18, max_threshold + step, step):\n",
    "        qb = QuickBundles(threshold=threshold)\n",
    "        clusters = qb.cluster(streamlines)\n",
    "        print(f\"Threshold: {threshold}, Number of clusters: {len(clusters)}\")\n",
    "        if len(clusters) == 3:\n",
    "            working_thresholds.append(threshold)\n",
    "    \n",
    "    return working_thresholds\n",
    "\n",
    "# Find all working thresholds for both CST tracts\n",
    "thresholds_right_all = find_all_thresholds_for_3_clusters(cst_right_streamlines)\n",
    "thresholds_left_all = find_all_thresholds_for_3_clusters(cst_left_streamlines)\n",
    "\n",
    "print(f\"CST Right - All thresholds that work: {thresholds_right_all}\")\n",
    "print(f\"CST Left - All thresholds that work: {thresholds_left_all}\")\n",
    "\n",
    "# Find common thresholds that work for both\n",
    "common_thresholds = list(set(thresholds_right_all) & set(thresholds_left_all))\n",
    "common_thresholds.sort()\n",
    "\n",
    "print(f\"Common thresholds that work for both: {common_thresholds}\")\n",
    "\n",
    "if common_thresholds:\n",
    "    largest_common_threshold = max(common_thresholds)\n",
    "    print(f\"Largest threshold that works for both CST tracts: {largest_common_threshold}\")\n",
    "    \n",
    "    # Verify with the largest common threshold\n",
    "    qb_common = QuickBundles(threshold=largest_common_threshold)\n",
    "    clusters_right_common = qb_common.cluster(cst_right_streamlines)\n",
    "    clusters_left_common = qb_common.cluster(cst_left_streamlines)\n",
    "    \n",
    "    print(f\"Verification with threshold {largest_common_threshold}:\")\n",
    "    print(f\"CST Right - Number of clusters: {len(clusters_right_common)}\")\n",
    "    print(f\"CST Left - Number of clusters: {len(clusters_left_common)}\")\n",
    "else:\n",
    "    print(\"No common thresholds found that work for both CST tracts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616a96e",
   "metadata": {},
   "source": [
    "### One central line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.io.streamline import load_tractogram\n",
    "from dipy.io.stateful_tractogram import StatefulTractogram,Space\n",
    "\n",
    "output_dir=\"/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/central_line\"\n",
    "threshold=100.\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all bundles\n",
    "for bundle_path in bundles:\n",
    "    print(f\"Processing {bundle_path} with threshold {threshold}\")\n",
    "    \n",
    "    # Load tractogram\n",
    "    tractogram = load_tractogram(bundle_path, reference='same',bbox_valid_check=False)\n",
    "    streamlines = tractogram.streamlines\n",
    "    \n",
    "    # Apply QuickBundles clustering\n",
    "    qb = QuickBundles(threshold=threshold)\n",
    "    clusters = qb.cluster(streamlines)\n",
    "    \n",
    "    # Extract centroids\n",
    "    centroids = [cluster.centroid for cluster in clusters]\n",
    "\n",
    "    \n",
    "    # Create a new tractogram with centroids\n",
    "    centroids_tractogram = StatefulTractogram(centroids, tractogram, Space.RASMM)\n",
    "    \n",
    "    # Save centroids as .trk file\n",
    "    bundle_name = bundle_path.split('/')[-1].replace('.trk', '')\n",
    "    print(f\"Bundle {bundle_name} - Number of clusters: {len(clusters)}\")\n",
    "\n",
    "    output_path = f\"{output_dir}/{bundle_name}_centroids.trk\"\n",
    "    \n",
    "    from dipy.io.streamline import save_tractogram\n",
    "    save_tractogram(centroids_tractogram, output_path)\n",
    "    \n",
    "    print(f\"Saved {len(centroids)} centroids to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a01abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "ref = '/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/average_anat.nii.gz'\n",
    "# Get all .trk files in the output directory\n",
    "trk_files = glob(f\"{output_dir}/*.trk\")\n",
    "out_flip_vtk = f\"/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/flipped/central_line\"\n",
    "\n",
    "print(f\"Found {len(trk_files)} .trk files in {output_dir}\")\n",
    "\n",
    "# Apply flip_tractogram command to each file\n",
    "for trk_file in trk_files:\n",
    "    out_vtk= f\"{out_flip_vtk}/{os.path.basename(trk_file).replace('.trk', '.vtk')}\"\n",
    "    print(f'Processing {trk_file}...')\n",
    "\n",
    "    # Run the flip_tractogram command\n",
    "    result = subprocess.run(['flip_tractogram', trk_file, out_vtk,'--reference',ref], \n",
    "                            capture_output=True, text=True, check=True)\n",
    "    print(f\"Successfully flipped {trk_file}\")\n",
    "\n",
    "\n",
    "print(\"Finished processing all files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf3ec6",
   "metadata": {},
   "source": [
    "## Long streamlines central line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dipy \n",
    "from glob import glob\n",
    "\n",
    "#Get all summed_*.trk from /home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/\n",
    "bundles = glob('/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/summed_*.trk')\n",
    "\n",
    "cst_right = [bundles for bundles in bundles if 'CST_right' in bundles][0]\n",
    "cst_left = [bundles for bundles in bundles if 'CST_left' in bundles][0]\n",
    "\n",
    "print(cst_right)\n",
    "print(cst_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.io.streamline import load_tractogram\n",
    "from dipy.io.stateful_tractogram import StatefulTractogram,Space\n",
    "import shapely\n",
    "from shapely import LineString, MultiLineString, Polygon\n",
    "output_dir=\"/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/long_central_line\"\n",
    "threshold=100.\n",
    "\n",
    "def get_longest_streamlines(streamlines, number=15):\n",
    "    \"\"\"\n",
    "    Trouve les N streamlines les plus longues.\n",
    "    Parameters\n",
    "    ----------\n",
    "    streamlines : list\n",
    "        Liste des streamlines (chaque streamline est un array numpy (N, 3))\n",
    "    number : int\n",
    "        Nombre de streamlines à retourner (par défaut 15)\n",
    "    Returns\n",
    "    -------\n",
    "    longest_streamlines : list\n",
    "        Liste des N streamlines les plus longues\n",
    "    \"\"\" \n",
    "    # Calculer la longueur de chaque streamline avec shapely\n",
    "    lengths = [LineString(s).length for s in streamlines]\n",
    "    \n",
    "    # Obtenir les indices des N plus longues\n",
    "    longest_indices = np.argsort(lengths)[-number:]\n",
    "    \n",
    "    # Retourner les streamlines correspondantes\n",
    "    longest_streamlines = [streamlines[i] for i in longest_indices]\n",
    "    \n",
    "    print(f\"Streamlines les plus longues : {len(longest_streamlines)}\")\n",
    "    return longest_streamlines\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all bundles\n",
    "for bundle_path in bundles:\n",
    "    print(f\"Processing {bundle_path} with threshold {threshold}\")\n",
    "    \n",
    "    # Load tractogram\n",
    "    tractogram = load_tractogram(bundle_path, reference='same',bbox_valid_check=False)\n",
    "    streamlines = tractogram.streamlines\n",
    "\n",
    "    streamlines = get_longest_streamlines(streamlines, number=int(0.1*len(streamlines)))\n",
    "    \n",
    "    # Apply QuickBundles clustering\n",
    "    qb = QuickBundles(threshold=threshold)\n",
    "    clusters = qb.cluster(streamlines)\n",
    "    \n",
    "    # Extract centroids\n",
    "    centroids = [cluster.centroid for cluster in clusters]\n",
    "\n",
    "    \n",
    "    # Create a new tractogram with centroids\n",
    "    centroids_tractogram = StatefulTractogram(centroids, tractogram, Space.RASMM)\n",
    "    \n",
    "    # Save centroids as .trk file\n",
    "    bundle_name = bundle_path.split('/')[-1].replace('.trk', '')\n",
    "    print(f\"Bundle {bundle_name} - Number of clusters: {len(clusters)}\")\n",
    "\n",
    "    output_path = f\"{output_dir}/{bundle_name}_centroids.trk\"\n",
    "    \n",
    "    from dipy.io.streamline import save_tractogram\n",
    "    save_tractogram(centroids_tractogram, output_path)\n",
    "    \n",
    "    print(f\"Saved {len(centroids)} centroids to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ae13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "ref = '/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/average_anat.nii.gz'\n",
    "# Get all .trk files in the output directory\n",
    "trk_files = glob(f\"{output_dir}/*.trk\")\n",
    "out_flip_vtk = f\"/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/long_central_line\"\n",
    "\n",
    "print(f\"Found {len(trk_files)} .trk files in {output_dir}\")\n",
    "\n",
    "# Apply flip_tractogram command to each file\n",
    "for trk_file in trk_files:\n",
    "    out_vtk= f\"{out_flip_vtk}/{os.path.basename(trk_file).replace('.trk', '.vtk')}\"\n",
    "    print(f'Processing {trk_file}...')\n",
    "\n",
    "    # Run the flip_tractogram command\n",
    "    result = subprocess.run(['flip_tractogram', trk_file, out_vtk,'--reference',ref], \n",
    "                            capture_output=True, text=True, check=True)\n",
    "    print(f\"Successfully flipped {trk_file}\")\n",
    "\n",
    "\n",
    "print(\"Finished processing all files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98cde1",
   "metadata": {},
   "source": [
    "## Multiple central lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.io.streamline import load_tractogram\n",
    "from dipy.io.stateful_tractogram import StatefulTractogram,Space\n",
    "\n",
    "output_dir=\"/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/flipped/centroids\"\n",
    "threshold=30.\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Dictionary to store bundle names and their cluster counts\n",
    "bundle_cluster_info = {}\n",
    "\n",
    "# Process all bundles\n",
    "for bundle_path in bundles:\n",
    "    print(f\"Processing {bundle_path} with threshold {threshold}\")\n",
    "    \n",
    "    # Load tractogram\n",
    "    tractogram = load_tractogram(bundle_path, reference='same',bbox_valid_check=False)\n",
    "    streamlines = tractogram.streamlines\n",
    "    \n",
    "    # Apply QuickBundles clustering\n",
    "    qb = QuickBundles(threshold=threshold)\n",
    "    clusters = qb.cluster(streamlines)\n",
    "    \n",
    "    # Extract centroids\n",
    "    centroids = [cluster.centroid for cluster in clusters]\n",
    "\n",
    "    \n",
    "    # Create a new tractogram with centroids\n",
    "    centroids_tractogram = StatefulTractogram(centroids, tractogram, Space.RASMM)\n",
    "    \n",
    "    # Save centroids as .trk file\n",
    "    bundle_name = bundle_path.split('/')[-1].replace('.trk', '')\n",
    "    print(f\"Bundle {bundle_name} - Number of clusters: {len(clusters)}\")\n",
    "    \n",
    "    # Store bundle info\n",
    "    bundle_cluster_info[bundle_name] = len(clusters)\n",
    "\n",
    "    output_path = f\"{output_dir}/{bundle_name}_centroids.vtk\"\n",
    "    \n",
    "    save_tractogram(centroids_tractogram, output_path)\n",
    "    \n",
    "    print(f\"Saved {len(centroids)} centroids to {output_path}\")\n",
    "\n",
    "# Save the bundle cluster information to JSON\n",
    "json_output_path = f\"{output_dir}/bundle_cluster_info.json\"\n",
    "with open(json_output_path, 'w') as json_file:\n",
    "    json.dump(bundle_cluster_info, json_file, indent=2)\n",
    "\n",
    "print(f\"Saved bundle cluster information to {json_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc15615",
   "metadata": {},
   "source": [
    "### Multiple clusters with Frechet distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae960971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle cluster information to /home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/flipped/centroids_frechet/bundle_cluster_info.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import vtk\n",
    "from vtk.util.numpy_support import numpy_to_vtk\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.segment.featurespeed import Feature\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.tracking.streamline import Streamlines, set_number_of_points\n",
    "from shapely.geometry import LineString\n",
    "import shapely\n",
    "\n",
    "def load_vtk_streamlines(vtk_file_path):\n",
    "    reader = vtk.vtkPolyDataReader()\n",
    "    reader.SetFileName(vtk_file_path)\n",
    "    reader.Update()\n",
    "    polydata = reader.GetOutput()\n",
    "    lines = polydata.GetLines()\n",
    "    streamlines = []\n",
    "    lines.InitTraversal()\n",
    "    id_list = vtk.vtkIdList()\n",
    "    while lines.GetNextCell(id_list):\n",
    "        line_points = []\n",
    "        for j in range(id_list.GetNumberOfIds()):\n",
    "            point_id = id_list.GetId(j)\n",
    "            point = polydata.GetPoint(point_id)\n",
    "            line_points.append(point)\n",
    "        streamlines.append(np.array(line_points))\n",
    "    return streamlines\n",
    "\n",
    "# --- Réorientation par distance de Manhattan avec la streamline moyenne ---\n",
    "def compute_mean_streamline(streamlines):\n",
    "    qb = QuickBundles(threshold=1000)\n",
    "    clusters = qb.cluster(streamlines)\n",
    "    return clusters.centroids[0]\n",
    "\n",
    "def reorient_to_reference_manhattan(streamlines, reference):\n",
    "    if reference is None:\n",
    "        return streamlines\n",
    "    oriented = []\n",
    "    for s in streamlines:\n",
    "        if len(s) == 0:\n",
    "            oriented.append(s)\n",
    "            continue\n",
    "        d_orig = np.sum(np.abs(s - reference))\n",
    "        s_rev = s[::-1]\n",
    "        d_flip = np.sum(np.abs(s_rev - reference))\n",
    "        oriented.append(s_rev if d_flip < d_orig else s)\n",
    "    return Streamlines(oriented)\n",
    "\n",
    "class FrechetDistanceFeature(Feature):\n",
    "    def infer_shape(self, datum):\n",
    "        return np.asarray(datum).shape\n",
    "    def extract(self, datum):\n",
    "        return np.asarray(datum)\n",
    "\n",
    "class FrechetDistanceMetric(Metric):\n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return True\n",
    "    def distance(self, feature1, feature2):\n",
    "        return shapely.hausdorff_distance(LineString(feature1), LineString(feature2))\n",
    "    def dist(self, features1, features2):\n",
    "        return self.distance(features1, features2)\n",
    "\n",
    "output_dir = \"/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/flipped/centroids_frechet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "threshold = 30.0\n",
    "\n",
    "feature = FrechetDistanceFeature()\n",
    "metric = FrechetDistanceMetric(feature=feature)\n",
    "\n",
    "bundle_cluster_info = {}\n",
    "bundles = glob('/home/ndecaux/NAS_EMPENN/share/projects/HCP105_Zenodo_NewTrkFormat/inGroupe1Space/Atlas/vtk/summed_*.vtk')\n",
    "\n",
    "for bundle_path in bundles:\n",
    "    print('\\n')\n",
    "    bundle_name = os.path.basename(bundle_path).replace('.vtk', '')\n",
    "    print(f\"---------------- Processing {bundle_name} --------------\")\n",
    "    sl = load_vtk_streamlines(bundle_path)\n",
    "    # Rééchantillonnage\n",
    "    sl = set_number_of_points(Streamlines(sl), 12)\n",
    "    # Calcul de la streamline moyenne et réorientation Manhattan\n",
    "    mean_ref = compute_mean_streamline(sl)\n",
    "    sl = reorient_to_reference_manhattan(sl, mean_ref)\n",
    "\n",
    "    # Statistiques longueur (optionnel)\n",
    "    lengths = []\n",
    "    for s in sl:\n",
    "        if len(s) > 0:\n",
    "            lengths.append(LineString(s).length)\n",
    "    if len(lengths) > 0:\n",
    "        mean_length = np.mean(lengths)\n",
    "        print(f\"Mean streamline length for {bundle_name}: {mean_length:.2f} mm\")\n",
    "\n",
    "    print(f\"Processing {bundle_name} with threshold {threshold} (Frechet)\")\n",
    "\n",
    "    max_clusters = 3\n",
    "\n",
    "    if bundle_name.replace('left', 'right') in bundle_cluster_info.keys():\n",
    "        max_clusters = bundle_cluster_info[bundle_name.replace('left', 'right')]\n",
    "    elif bundle_name.replace('right', 'left') in bundle_cluster_info.keys():\n",
    "        max_clusters = bundle_cluster_info[bundle_name.replace('right', 'left')]\n",
    "\n",
    "    print(f\"Max clusters for {bundle_name}: {max_clusters}\")\n",
    "    \n",
    "\n",
    "\n",
    "    qb = QuickBundles(threshold=threshold, metric=metric,max_nb_clusters=max_clusters)\n",
    "    clusters = qb.cluster(sl)\n",
    "\n",
    "\n",
    "    #If the number of streamlines in a centroid is less than 1000, remove it\n",
    "    bad_clusters = clusters.get_small_clusters(1000)\n",
    "    for bad_cluster in bad_clusters:\n",
    "        print(f\"Removing bad cluster with {len(bad_cluster.indices)} streamlines for {bundle_name}\")\n",
    "        clusters.remove_cluster(bad_cluster)\n",
    "\n",
    "\n",
    "    centroids = clusters.centroids\n",
    "\n",
    "\n",
    "    print(f\"Bundle {bundle_name} - Number of clusters (Frechet): {len(clusters)}\")\n",
    "    bundle_cluster_info[bundle_name] = len(clusters)\n",
    "\n",
    "    # Ecriture des centroids en VTK\n",
    "    centroid_polydata = vtk.vtkPolyData()\n",
    "    centroid_points = vtk.vtkPoints()\n",
    "    centroid_lines = vtk.vtkCellArray()\n",
    "    centroid_polydata.SetPoints(centroid_points)\n",
    "    centroid_indices = []\n",
    "    for cid, c in enumerate(centroids):\n",
    "        line = vtk.vtkPolyLine()\n",
    "        line.GetPointIds().SetNumberOfIds(len(c))\n",
    "        for i, p in enumerate(c):\n",
    "            pid = centroid_points.InsertNextPoint(float(p[0]), float(p[1]), float(p[2]))\n",
    "            line.GetPointIds().SetId(i, pid)\n",
    "            centroid_indices.append(cid)\n",
    "        centroid_lines.InsertNextCell(line)\n",
    "    centroid_polydata.SetLines(centroid_lines)\n",
    "    centroid_index_array = numpy_to_vtk(np.array(centroid_indices), deep=True)\n",
    "    centroid_index_array.SetName('centroid_index')\n",
    "    centroid_polydata.GetPointData().AddArray(centroid_index_array)\n",
    "    centroid_writer = vtk.vtkPolyDataWriter()\n",
    "    centroid_writer.SetFileName(os.path.join(output_dir, f\"{bundle_name}_centroids.vtk\"))\n",
    "    centroid_writer.SetInputData(centroid_polydata)\n",
    "    centroid_writer.Write()\n",
    "\n",
    "    # Ecriture du modèle avec centroid_index et point_index\n",
    "    model_polydata = vtk.vtkPolyData()\n",
    "    model_points = vtk.vtkPoints()\n",
    "    model_lines = vtk.vtkCellArray()\n",
    "    model_polydata.SetPoints(model_points)\n",
    "\n",
    "    # Mapping streamline -> cluster id\n",
    "    streamline_cluster_ids = np.full(len(sl), -1, dtype=int)\n",
    "    for cid, c in enumerate(clusters):\n",
    "        for sidx in c.indices:\n",
    "            streamline_cluster_ids[sidx] = cid\n",
    "\n",
    "    model_centroid_indices = []\n",
    "    model_point_indices = []\n",
    "    for sidx, s in enumerate(sl):\n",
    "        line = vtk.vtkPolyLine()\n",
    "        line.GetPointIds().SetNumberOfIds(len(s))\n",
    "        for i, p in enumerate(s):\n",
    "            pid = model_points.InsertNextPoint(float(p[0]), float(p[1]), float(p[2]))\n",
    "            line.GetPointIds().SetId(i, pid)\n",
    "            model_centroid_indices.append(streamline_cluster_ids[sidx])\n",
    "            model_point_indices.append(i)\n",
    "        model_lines.InsertNextCell(line)\n",
    "    model_polydata.SetLines(model_lines)\n",
    "    arr_centroid_index = numpy_to_vtk(np.array(model_centroid_indices), deep=True)\n",
    "    arr_centroid_index.SetName('centroid_index')\n",
    "    model_polydata.GetPointData().AddArray(arr_centroid_index)\n",
    "    arr_point_index = numpy_to_vtk(np.array(model_point_indices), deep=True)\n",
    "    arr_point_index.SetName('point_index')\n",
    "    model_polydata.GetPointData().AddArray(arr_point_index)\n",
    "    model_writer = vtk.vtkPolyDataWriter()\n",
    "    model_writer.SetFileName(os.path.join(output_dir, f\"{bundle_name}_model_with_centroid_index.vtk\"))\n",
    "    model_writer.SetInputData(model_polydata)\n",
    "    model_writer.Write()\n",
    "\n",
    "json_output_path = os.path.join(output_dir, 'bundle_cluster_info.json')\n",
    "with open(json_output_path, 'w') as f:\n",
    "    json.dump(bundle_cluster_info, f, indent=2)\n",
    "print(f\"Saved bundle cluster information to {json_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179313db",
   "metadata": {},
   "source": [
    "### Test du pipeline d'association HCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafda680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du pipeline d'association avec un sujet\n",
    "import sys\n",
    "sys.path.append('/home/ndecaux/Code/actiDep')\n",
    "\n",
    "from actiDep.pipeline.hcp_association import process_hcp_association, get_bundle_mapping\n",
    "from actiDep.data.loader import Subject, Actidep\n",
    "\n",
    "# Tester le mapping des bundles\n",
    "bundle_mapping = get_bundle_mapping()\n",
    "print(\"Bundle mapping disponible :\")\n",
    "for bundle, centroid in bundle_mapping.items():\n",
    "    print(f\"  {bundle} -> {centroid}\")\n",
    "\n",
    "# Tester avec un sujet\n",
    "ds = Actidep('/home/ndecaux/NAS_EMPENN/share/projects/actidep/bids')\n",
    "if len(ds.subject_ids) > 0:\n",
    "    subject = ds.get_subject(ds.subject_ids[0])\n",
    "    \n",
    "    # Vérifier les fichiers VTK disponibles\n",
    "    vtk_files = subject.get(\n",
    "        pipeline='mcm_to_hcp_space',\n",
    "        space='HCP', \n",
    "        extension='vtk',\n",
    "        datatype='tracto'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFichiers VTK trouvés pour {subject.sub_id}: {len(vtk_files)}\")\n",
    "    for vtk_file in vtk_files:\n",
    "        entities = vtk_file.get_entities()\n",
    "        print(f\"  Bundle: {entities.get('bundle', 'unknown')}\")\n",
    "    \n",
    "    # Lancer le pipeline sur ce sujet\n",
    "    if len(vtk_files) > 0:\n",
    "        print(f\"\\nTraitement du sujet {subject.sub_id}...\")\n",
    "        process_hcp_association(subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
